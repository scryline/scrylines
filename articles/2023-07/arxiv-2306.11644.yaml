{
  "uuid": "f58f5078-efeb-4028-989b-6aa4883215e4",
  "version": "0.1",
  "sources": [
    {
      "id": "2306.11644",
      "url": "https://arxiv.org/abs/2306.11644",
      "site": "arxiv"
    }
  ],
  "components": [
    {
      "source_url": "https://arxiv.org/abs/2306.11644",
      "type": "abstract",
      "content": "Textbooks Are All You Need\n\nAuthors:Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, Yuanzhi Li\n\nWe introduce phi-1, a new large language model for code, with significantly\nsmaller size than competing models: phi-1 is a Transformer-based model with\n1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook\nquality\" data from the web (6B tokens) and synthetically generated textbooks\nand exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays\nsurprising emergent properties compared to phi-1-base, our model before our\nfinetuning stage on a dataset of coding exercises, and phi-1-small, a smaller\nmodel with 350M parameters trained with the same pipeline as phi-1 that still\nachieves 45% on HumanEval.\n\n                                         Skip to main content       We are hiring   We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate        >  cs  > arXiv:2306.11644       Help | Advanced Search      All fields  Title  Author  Abstract  Comments  Journal reference  ACM classification  MSC classification  Report number  arXiv identifier  DOI  ORCID  arXiv author ID  Help pages  Full text      Search                        GO        quick links   Login  Help Pages  About               Computer Science > Computation and Language    arXiv:2306.11644 (cs)     [Submitted on 20 Jun 2023]  Title: Textbooks Are All You Need  Authors: Suriya Gunasekar , Yi Zhang , Jyoti Aneja , Caio C\u00e9sar Teodoro Mendes , Allie Del Giorno , Sivakanth Gopi , Mojan Javaheripi , Piero Kauffmann , Gustavo de Rosa , Olli Saarikivi , Adil Salim , Shital Shah , Harkirat Singh Behl , Xin Wang , S\u00e9bastien Bubeck , Ronen Eldan , Adam Tauman Kalai , Yin Tat Lee , Yuanzhi Li  Download a PDF of the paper titled Textbooks Are All You Need, by Suriya Gunasekar and 18 other authors  Download PDF   Abstract: We introduce phi-1, a new large language model for code, with significantly\nsmaller size than competing models: phi-1 is a Transformer-based model with\n1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook\nquality\" data from the web (6B tokens) and synthetically generated textbooks\nand exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays\nsurprising emergent properties compared to phi-1-base, our model before our\nfinetuning stage on a dataset of coding exercises, and phi-1-small, a smaller\nmodel with 350M parameters trained with the same pipeline as phi-1 that still\nachieves 45% on HumanEval.      Comments:  26 pages    Subjects:   Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)   Cite as:  arXiv:2306.11644 [cs.CL]      (or  arXiv:2306.11644v1 [cs.CL] for this version)       https://doi.org/10.48550/arXiv.2306.11644     Focus to learn more     arXiv-issued DOI via DataCite         Submission history From: Suriya Gunasekar [ view email ]  [v1] Tue, 20 Jun 2023 16:14:25 UTC (830 KB)        Full-text links:  Download:   Download a PDF of the paper titled Textbooks Are All You Need, by Suriya Gunasekar and 18 other authors  PDF  Other formats     Current browse context: cs.CL    <\u00a0prev   |   next\u00a0>    new  |  recent  |  2306  Change to browse by:  cs  cs.AI  cs.LG      References & Citations   NASA ADS Google Scholar  Semantic Scholar       3 blog links ( what is this? )    a  export BibTeX citation  Loading...      BibTeX formatted citation  \u00d7    loading...    Data provided by:       Bookmark             Bibliographic Tools   Bibliographic and Citation Tools        Bibliographic Explorer Toggle     Bibliographic Explorer  ( What is the Explorer? )         Litmaps Toggle     Litmaps  ( What is Litmaps? )         scite.ai Toggle     scite Smart Citations  ( What are Smart Citations? )          Code, Data, Media   Code, Data and Media Associated with this Article        Links to Code Toggle     CatalyzeX Code Finder for Papers  ( What is CatalyzeX? )         DagsHub Toggle     DagsHub  ( What is DagsHub? )         Links to Code Toggle     Papers with Code  ( What is Papers with Code? )         ScienceCast Toggle     ScienceCast  ( What is ScienceCast? )            Demos   Demos        Replicate Toggle     Replicate  ( What is Replicate? )         Spaces Toggle     Hugging Face Spaces  ( What is Spaces? )         Related Papers   Recommenders and Search Tools        Link to Influence Flower     Influence Flower  ( What are Influence Flowers? )         Connected Papers Toggle     Connected Papers  ( What is Connected Papers? )         Core recommender toggle     CORE Recommender  ( What is CORE? )       Author  Venue  Institution  Topic               About arXivLabs     arXivLabs: experimental projects with community collaborators  arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.  Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.  Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs .            Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )              About  Help       Click here to contact arXiv  Contact    Click here to subscribe  Subscribe             Copyright  Privacy Policy      Web Accessibility Assistance    arXiv Operational Status Get status notifications via email or slack              "
    },
    {
      "source_url": "https://arxiv.org/abs/2306.11644",
      "type": "generated",
      "content": "Textbooks Are All You Need: New Research Paper Introduces phi-1 Language Model\n\nBy [Author's Name]\n\n[City, State] - A group of researchers, led by Suriya Gunasekar, has recently published a groundbreaking research paper titled \"Textbooks Are All You Need.\" The paper introduces a new large language model for code called phi-1, which boasts significantly smaller size compared to its competitors. The research team includes notable names such as Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li.\n\nThe research paper, available on arXiv at [insert hyperlink to the paper], presents the phi-1 language model, a Transformer-based model with 1.3 billion parameters. The model was trained for four days on eight A100s, utilizing a combination of \"textbook quality\" data from the web, consisting of 6 billion tokens, and synthetically generated textbooks and exercises using GPT-3.5, which added another 1 billion tokens to the training corpus.\n\nDespite its relatively small scale, phi-1 has achieved remarkable results. The model attains a pass@1 accuracy of 50.6% on HumanEval and 55.5% on MBPP, showcasing its ability to accurately comprehend and generate code. Furthermore, phi-1 exhibits surprising emergent properties when compared to phi-1-base, the model before the finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350 million parameters trained using the same pipeline as phi-1, which still achieves an impressive 45% accuracy on HumanEval.\n\nThe introduction of phi-1 addresses the need for a more compact language model for code, as existing models often come with larger sizes and computational requirements. The researchers' approach of utilizing \"textbook quality\" data and synthetic textbooks and exercises demonstrates the potential for training language models on curated and structured content.\n\nGunasekar and the team believe that the phi-1 language model has the potential to revolutionize the field of code generation and comprehension. Its smaller size, combined with its competitive accuracy, opens up possibilities for more efficient and accessible code-related applications.\n\nThe research paper has garnered significant attention within the tech and AI communities. Experts are excited about the implications of phi-1 and its potential to enhance code-related tasks, such as automated code completion, bug detection, and code refactoring.\n\nAs the field of natural language processing continues to evolve, phi-1 stands as a testament to the power of language models and their ability to transform various domains. Gunasekar and the team's research paves the way for future advancements in code generation and comprehension, offering a promising outlook for developers and researchers alike.\n\nTo access the full research paper, \"Textbooks Are All You Need,\" by Suriya Gunasekar et al., please visit [insert hyperlink to the paper].\n\n###\n\nAbout the Authors:\n- Suriya Gunasekar\n- Yi Zhang\n- Jyoti Aneja\n- Caio C\u00e9sar Teodoro Mendes\n- Allie Del Giorno\n- Sivakanth Gopi\n- Mojan Javaheripi\n- Piero Kauffmann\n- Gustavo de Rosa\n- Olli Saarikivi\n- Adil Salim\n- Shital Shah\n- Harkirat Singh Behl\n- Xin Wang\n- S\u00e9bastien Bubeck\n- Ronen Eldan\n- Adam Tauman Kalai\n- Yin Tat Lee\n- Yuanzhi Li"
    },
    {
      "source_url": "https://arxiv.org/abs/2306.11644",
      "type": "facts",
      "content": [
        {
          "fact": "\"The paper introduces a new large language model for code called phi-1.\"",
          "source": "https://arxiv.org/abs/2306.11644",
          "check": true
        },
        {
          "fact": "\"The model was trained for four days on eight A100s.\"",
          "source": "https://arxiv.org/abs/2306.11644",
          "check": true
        },
        {
          "fact": "\"The model attains a pass@1 accuracy of 50.6% on HumanEval and 55.5% on MBPP.\"",
          "source": "https://arxiv.org/abs/2306.11644",
          "check": true
        },
        {
          "fact": "\"The researchers' approach of utilizing 'textbook quality' data and synthetic textbooks and exercises demonstrates the potential for training language models on curated and structured content.\"",
          "source": "https://arxiv.org/abs/2306.11644",
          "check": true
        },
        {
          "fact": "\"The research paper has garnered significant attention within the tech and AI communities.\"",
          "source": "https://arxiv.org/abs/2306.11644",
          "check": true
        }
      ]
    },
    {
      "source_url": "",
      "type": "annotated",
      "content": "Textbooks Are All You Need: New Research Paper Introduces phi-1 Language Model\n\nBy [Author's Name]\n\n[City, State] - A group of researchers, led by Suriya Gunasekar, has recently published a groundbreaking research paper titled \"Textbooks Are All You Need.[\" The paper introduces a new large language model for code called phi-1, which boasts significantly smaller size compared to its competitors](https://arxiv.org/abs/2306.11644). The research team includes notable names such as Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li.\n\nThe research paper, available on arXiv at [insert hyperlink to the paper], presents the phi-1 language model, a Transformer-based model with 1.3 billion parameters. [The model was trained for four days on eight A100s, utilizing a combination of \"textbook quality\" data from the web, consisting of 6 billion tokens, and synthetically generated textbooks and exercises using GPT-3](https://arxiv.org/abs/2306.11644).[5, which added another 1 billion tokens to the training corpus](https://arxiv.org/abs/2306.11644).\n\nDespite its relatively small scale, phi-1 has achieved remarkable results. The model attains a pass@1 accuracy of 50.6% on HumanEval and 55.5% on MBPP, showcasing its ability to accurately comprehend and generate code. Furthermore, phi-1 exhibits surprising emergent properties when compared to phi-1-base, the model before the finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350 million parameters trained using the same pipeline as phi-1, which still achieves an impressive 45% accuracy on HumanEval.\n\nThe introduction of phi-1 addresses the need for a more compact language model for code, as existing models often come with larger sizes and computational requirements. The researchers' approach of utilizing \"textbook quality\" data and synthetic textbooks and exercises demonstrates the potential for training language models on curated and structured content.\n\n[Gunasekar and the team believe that the phi-1 language model has the potential to revolutionize the field of code generation and comprehension](https://arxiv.org/abs/2306.11644). Its smaller size, combined with its competitive accuracy, opens up possibilities for more efficient and accessible code-related applications.\n\nThe research paper has garnered significant attention within the tech and AI communities. Experts are excited about the implications of phi-1 and its potential to enhance code-related tasks, such as automated code completion, bug detection, and code refactoring.\n\nAs the field of natural language processing continues to evolve, phi-1 stands as a testament to the power of language models and their ability to transform various domains. Gunasekar and the team's research paves the way for future advancements in code generation and comprehension, offering a promising outlook for developers and researchers alike.\n\n[To access the full research paper, \"Textbooks Are All You Need,\" by Suriya Gunasekar et al](https://arxiv.org/abs/2306.11644)., please visit [insert hyperlink to the paper].\n\n###\n\nAbout the Authors:\n- Suriya Gunasekar\n- Yi Zhang\n- Jyoti Aneja\n- Caio C\u00e9sar Teodoro Mendes\n- Allie Del Giorno\n- Sivakanth Gopi\n- Mojan Javaheripi\n- Piero Kauffmann\n- Gustavo de Rosa\n- Olli Saarikivi\n- Adil Salim\n- Shital Shah\n- Harkirat Singh Behl\n- Xin Wang\n- S\u00e9bastien Bubeck\n- Ronen Eldan\n- Adam Tauman Kalai\n- Yin Tat Lee\n- Yuanzhi Li"
    }
  ],
  "filename": "arxiv-2306.11644"
}